PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 1600)
        (wpe): Embedding(1024, 1600)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-43): 44 x GPT2Block(
            (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2FlashAttention2(
              (c_attn): lora.Linear(
                (base_layer): Conv1D(nf=4800, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_0_43): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=1600, out_features=64, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=64, out_features=4800, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_0_43): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=1600, out_features=64, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=64, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=6400, nx=1600)
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=6400)
                (lora_dropout): ModuleDict(
                  (layers_0_43): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=6400, out_features=64, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_0_43): ModuleDict(
                    (0): Linear(in_features=64, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (44): GPT2Block(
            (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2FlashAttention2(
              (c_attn): lora.Linear(
                (base_layer): Conv1D(nf=4800, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_44): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=1600, out_features=23, bias=False)
                    (1): Linear(in_features=1600, out_features=41, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=23, out_features=4800, bias=False)
                    (1): Linear(in_features=41, out_features=4800, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_44): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=1600, out_features=23, bias=False)
                    (1): Linear(in_features=1600, out_features=41, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=23, out_features=1600, bias=False)
                    (1): Linear(in_features=41, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=6400, nx=1600)
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=6400)
                (lora_dropout): ModuleDict(
                  (layers_44): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=6400, out_features=23, bias=False)
                    (1): Linear(in_features=6400, out_features=41, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_44): ModuleDict(
                    (0): Linear(in_features=23, out_features=1600, bias=False)
                    (1): Linear(in_features=41, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (45): GPT2Block(
            (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2FlashAttention2(
              (c_attn): lora.Linear(
                (base_layer): Conv1D(nf=4800, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_45): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=1600, out_features=14, bias=False)
                    (1): Linear(in_features=1600, out_features=9, bias=False)
                    (2): Linear(in_features=1600, out_features=7, bias=False)
                    (3): Linear(in_features=1600, out_features=34, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=14, out_features=4800, bias=False)
                    (1): Linear(in_features=9, out_features=4800, bias=False)
                    (2): Linear(in_features=7, out_features=4800, bias=False)
                    (3): Linear(in_features=34, out_features=4800, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_45): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=1600, out_features=14, bias=False)
                    (1): Linear(in_features=1600, out_features=9, bias=False)
                    (2): Linear(in_features=1600, out_features=7, bias=False)
                    (3): Linear(in_features=1600, out_features=34, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=14, out_features=1600, bias=False)
                    (1): Linear(in_features=9, out_features=1600, bias=False)
                    (2): Linear(in_features=7, out_features=1600, bias=False)
                    (3): Linear(in_features=34, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=6400, nx=1600)
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=6400)
                (lora_dropout): ModuleDict(
                  (layers_45): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=6400, out_features=14, bias=False)
                    (1): Linear(in_features=6400, out_features=9, bias=False)
                    (2): Linear(in_features=6400, out_features=7, bias=False)
                    (3): Linear(in_features=6400, out_features=34, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_45): ModuleDict(
                    (0): Linear(in_features=14, out_features=1600, bias=False)
                    (1): Linear(in_features=9, out_features=1600, bias=False)
                    (2): Linear(in_features=7, out_features=1600, bias=False)
                    (3): Linear(in_features=34, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (46-47): 2 x GPT2Block(
            (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2FlashAttention2(
              (c_attn): lora.Linear(
                (base_layer): Conv1D(nf=4800, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_46_47): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=1600, out_features=14, bias=False)
                    (1): Linear(in_features=1600, out_features=9, bias=False)
                    (2): Linear(in_features=1600, out_features=7, bias=False)
                    (3): Linear(in_features=1600, out_features=31, bias=False)
                    (4): Linear(in_features=1600, out_features=3, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=14, out_features=4800, bias=False)
                    (1): Linear(in_features=9, out_features=4800, bias=False)
                    (2): Linear(in_features=7, out_features=4800, bias=False)
                    (3): Linear(in_features=31, out_features=4800, bias=False)
                    (4): Linear(in_features=3, out_features=4800, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=1600)
                (lora_dropout): ModuleDict(
                  (layers_46_47): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=1600, out_features=14, bias=False)
                    (1): Linear(in_features=1600, out_features=9, bias=False)
                    (2): Linear(in_features=1600, out_features=7, bias=False)
                    (3): Linear(in_features=1600, out_features=31, bias=False)
                    (4): Linear(in_features=1600, out_features=3, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=14, out_features=1600, bias=False)
                    (1): Linear(in_features=9, out_features=1600, bias=False)
                    (2): Linear(in_features=7, out_features=1600, bias=False)
                    (3): Linear(in_features=31, out_features=1600, bias=False)
                    (4): Linear(in_features=3, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=6400, nx=1600)
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=1600, nx=6400)
                (lora_dropout): ModuleDict(
                  (layers_46_47): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=6400, out_features=14, bias=False)
                    (1): Linear(in_features=6400, out_features=9, bias=False)
                    (2): Linear(in_features=6400, out_features=7, bias=False)
                    (3): Linear(in_features=6400, out_features=31, bias=False)
                    (4): Linear(in_features=6400, out_features=3, bias=False)
                  )
                )
                (lora_B): ModuleDict(
                  (layers_46_47): ModuleDict(
                    (0): Linear(in_features=14, out_features=1600, bias=False)
                    (1): Linear(in_features=9, out_features=1600, bias=False)
                    (2): Linear(in_features=7, out_features=1600, bias=False)
                    (3): Linear(in_features=31, out_features=1600, bias=False)
                    (4): Linear(in_features=3, out_features=1600, bias=False)
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=1600, out_features=50257, bias=False)
    )
  )
)